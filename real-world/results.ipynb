{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\"EDC\", \"random_forest\", \"logistic_regression\", \"svm_linear\", \"svm_rbf\", \"decision_tree\"]\n",
    "score_metric = \"auc_score\"\n",
    "score_metric_index = 0 if score_metric == \"auc_score\" else 1\n",
    "\n",
    "result_files = os.listdir(\"results\")\n",
    "results = []\n",
    "for result_file in result_files:\n",
    "    dataset, search_strategy, optimiser, random_seed = result_file.split(\n",
    "        \".\"\n",
    "    )[0].split(\"-\")\n",
    "    with open(f\"results/{result_file}\", \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "    for elapsed_time, auc_score, accuracy_score in zip(res[\"elapsed_time\"], res[\"auc_score\"], res[\"accuracy_score\"]):\n",
    "        results.append(\n",
    "            [\n",
    "                dataset,\n",
    "                \"EDC\",  # \"classifier\n",
    "                search_strategy,\n",
    "                optimiser,\n",
    "                elapsed_time,\n",
    "                auc_score,\n",
    "                accuracy_score,\n",
    "                random_seed,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "result_files = os.listdir(\"comparison_results\")\n",
    "for result_file in result_files:\n",
    "    dataset, classifier, random_seed = result_file.split(\n",
    "        \".\"\n",
    "    )[0].split(\"-\")\n",
    "    with open(f\"comparison_results/{result_file}\", \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "    for elapsed_time, auc_score, accuracy_score in zip(res[\"elapsed_time\"], res[\"auc_score\"], res[\"accuracy_score\"]):\n",
    "        results.append(\n",
    "            [\n",
    "                dataset,\n",
    "                classifier,  # \"classifier\n",
    "                None,\n",
    "                None,\n",
    "                elapsed_time,\n",
    "                auc_score,\n",
    "                accuracy_score,\n",
    "                random_seed,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"classifier\",\n",
    "        \"search_strategy\",\n",
    "        \"optimiser\",\n",
    "        \"elapsed_time\",\n",
    "        \"auc_score\",\n",
    "        \"accuracy_score\",\n",
    "        \"random_seed\"\n",
    "    ],\n",
    "    data=results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'EDC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Get the ranks\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m classifier \u001b[38;5;129;01min\u001b[39;00m classifiers:\n\u001b[0;32m---> 25\u001b[0m     current_score \u001b[38;5;241m=\u001b[39m \u001b[43mscores\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m]\u001b[49m[score_metric_index]\n\u001b[1;32m     26\u001b[0m     rank \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m other_classifier \u001b[38;5;129;01min\u001b[39;00m classifiers:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'EDC'"
     ]
    }
   ],
   "source": [
    "dataset_grouped = df.groupby([\"dataset\"])\n",
    "all_scores = []\n",
    "all_stds = []\n",
    "for dataset_group in dataset_grouped:\n",
    "    scores = {}\n",
    "    stds = {}\n",
    "    dataset = dataset_group[0][0]\n",
    "    if dataset == \"BANANA\" or (dataset.startswith(\"AD0\")):\n",
    "        continue\n",
    "    for classifier_group in dataset_group[1].groupby([\"classifier\"]):\n",
    "        number_of_folds_done = len(classifier_group[1])\n",
    "\n",
    "        classifier = classifier_group[0][0]\n",
    "        mean_auc = classifier_group[1][\"auc_score\"].mean()\n",
    "        std_auc = classifier_group[1][\"auc_score\"].std()\n",
    "        mean_acc = classifier_group[1][\"accuracy_score\"].mean()\n",
    "        std_acc = classifier_group[1][\"accuracy_score\"].std()\n",
    "        scores[classifier] = (mean_auc, mean_acc)\n",
    "        stds[classifier] = (std_auc, std_acc)\n",
    "        \n",
    "\n",
    "\n",
    "    # Get the ranks\n",
    "    for classifier in classifiers:\n",
    "        current_score = scores[classifier][score_metric_index]\n",
    "        rank = 1\n",
    "        for other_classifier in classifiers:\n",
    "            if other_classifier == classifier:\n",
    "                continue\n",
    "            if scores[other_classifier][score_metric_index] > current_score:\n",
    "                rank += 1\n",
    "        scores[classifier] = (scores[classifier][0], scores[classifier][1], rank)\n",
    "    \n",
    "\n",
    "    print(f\"{dataset}\", end=\"&\")\n",
    "    if score_metric == \"auc_score\":\n",
    "        # Print AUC scores\n",
    "        for clf in classifiers:\n",
    "            print(f\"{scores[clf][0]:.4f} ($\\\\pm{stds[clf][0]:.2f}$)\", end=\"&\")\n",
    "\n",
    "    if score_metric == \"accuracy_score\":\n",
    "        # Print Acc\n",
    "        for clf in classifiers:\n",
    "            print(f\"{scores[clf][1]:.4f} ($\\\\pm{stds[clf][1]:.2f}$)\", end=\"&\")\n",
    "    \n",
    "    # # Print rank\n",
    "    # for clf in classifiers:\n",
    "    #     print(f\"{scores[clf][2]}\", end=\"&\")\n",
    "    \n",
    "    print(f\"\\b\\\\\\\\\")\n",
    "\n",
    "    all_scores.append(scores)\n",
    "    all_stds.append(stds)\n",
    "    # print(f\"{dataset} & {scores['EDC'][0]:.4f} & {scores['random_forest'][0]:.4f} & {scores['EDC'][1]:.4f} & {scores['random_forest'][1]:.4f} \\\\\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "# Get average ranks\n",
    "average_ranks = {}\n",
    "average_aucs = {}\n",
    "average_accs = {}\n",
    "for classifier in classifiers:\n",
    "    average_ranks[classifier] = 0\n",
    "    average_aucs[classifier] = 0\n",
    "    average_accs[classifier] = 0\n",
    "    for dataset in all_scores:\n",
    "        if classifier not in dataset:\n",
    "            continue\n",
    "        average_ranks[classifier] += dataset[classifier][2]\n",
    "        average_aucs[classifier] += dataset[classifier][0]\n",
    "        average_accs[classifier] += dataset[classifier][1]\n",
    "    average_ranks[classifier] /= len(all_scores)\n",
    "    average_aucs[classifier] /= len(all_scores)\n",
    "    average_accs[classifier] /= len(all_scores)\n",
    "\n",
    "print(\"\\\\midrule\")\n",
    "print(f\"Average Score&\", end=\"\")\n",
    "for classifier in classifiers:\n",
    "    print(f\"{average_aucs[classifier]:.4f}\", end=\"&\")\n",
    "print(f\"\\b\\\\\\\\\")\n",
    "\n",
    "print(f\"Average Rank&\", end=\"\")\n",
    "for classifier in classifiers:\n",
    "    print(f\"{average_ranks[classifier]:.2f}\", end=\"&\")\n",
    "print(f\"\\b\\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
