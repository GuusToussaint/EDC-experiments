{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\"EDC\", \"AMAXSC\", \"M4GP\", \"lda\",  \"decision_tree\", \"MLP\", \"random_forest\", \"svm_rbf\"]\n",
    "score_metric = \"auc_score\"\n",
    "score_metric_index = 0 if score_metric == \"auc_score\" else 1\n",
    "\n",
    "result_files = os.listdir(\"results\")\n",
    "results = []\n",
    "for result_file in result_files:\n",
    "    dataset, search_strategy, optimiser, random_seed = result_file.split(\n",
    "        \".\"\n",
    "    )[0].split(\"-\")\n",
    "    with open(f\"results/{result_file}\", \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "    for elapsed_time, auc_score, accuracy_score in zip(res[\"elapsed_time\"], res[\"auc_score\"], res[\"accuracy_score\"]):\n",
    "        results.append(\n",
    "            [\n",
    "                dataset,\n",
    "                \"EDC\",  # \"classifier\n",
    "                search_strategy,\n",
    "                optimiser,\n",
    "                elapsed_time,\n",
    "                auc_score,\n",
    "                accuracy_score,\n",
    "                random_seed,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "result_files = os.listdir(\"comparison_results\")\n",
    "for result_file in result_files:\n",
    "    dataset, classifier, random_seed = result_file.split(\n",
    "        \".\"\n",
    "    )[0].split(\"-\")\n",
    "    with open(f\"comparison_results/{result_file}\", \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "    for elapsed_time, auc_score, accuracy_score in zip(res[\"elapsed_time\"], res[\"auc_score\"], res[\"accuracy_score\"]):\n",
    "        results.append(\n",
    "            [\n",
    "                dataset,\n",
    "                classifier,  # \"classifier\n",
    "                None,\n",
    "                None,\n",
    "                elapsed_time,\n",
    "                auc_score,\n",
    "                accuracy_score,\n",
    "                random_seed,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"classifier\",\n",
    "        \"search_strategy\",\n",
    "        \"optimiser\",\n",
    "        \"elapsed_time\",\n",
    "        \"auc_score\",\n",
    "        \"accuracy_score\",\n",
    "        \"random_seed\"\n",
    "    ],\n",
    "    data=results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       dataset classifier search_strategy     optimiser  elapsed_time  \\\n",
      "0     BANKNOTE        EDC            beam  hill_climber      1.203219   \n",
      "1       BREAST        EDC            beam  hill_climber     38.952624   \n",
      "2     DIABETES        EDC            beam  hill_climber      2.500993   \n",
      "3       CREDIT        EDC            beam  hill_climber     42.151524   \n",
      "4    OCCUPANCY        EDC            beam  hill_climber      2.051289   \n",
      "..         ...        ...             ...           ...           ...   \n",
      "858  OCCUPANCY       M4GP            None          None   9687.420261   \n",
      "859  OCCUPANCY       M4GP            None          None   7649.649928   \n",
      "860  OCCUPANCY       M4GP            None          None   9140.788291   \n",
      "861  OCCUPANCY       M4GP            None          None   9044.762067   \n",
      "862      ADULT       M4GP            None          None  69212.286039   \n",
      "\n",
      "     auc_score  accuracy_score random_seed  \n",
      "0     1.000000        0.992754   1805819_0  \n",
      "1     0.605556        0.689655   1805819_0  \n",
      "2     0.798519        0.753247   1805819_0  \n",
      "3     0.871171        0.805970   1805819_0  \n",
      "4     0.996983        0.989300   1805819_0  \n",
      "..         ...             ...         ...  \n",
      "858   0.989672        0.986381  3921091577  \n",
      "859   0.991574        0.992704  3921091577  \n",
      "860   0.987779        0.986868  3921091577  \n",
      "861   0.990309        0.990759  3921091577  \n",
      "862   0.769552        0.828250  3318125531  \n",
      "\n",
      "[863 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADULT&0.8889 ($\\pm0.00$)&0.8070 ($\\pm0.01$)&0.7696 ($\\pmnan$)&0.9017 ($\\pm0.00$)&0.7301 ($\\pm0.01$)&0.9011 ($\\pm0.01$)&0.8799 ($\\pm0.00$)&0.8980 ($\\pm0.00$)\\\\\n",
      "BANKNOTE&1.0000 ($\\pm0.00$)&0.9815 ($\\pm0.02$)&0.9994 ($\\pm0.00$)&0.9996 ($\\pm0.00$)&0.9790 ($\\pm0.01$)&1.0000 ($\\pm0.00$)&0.9999 ($\\pm0.00$)&1.0000 ($\\pm0.00$)\\\\\n",
      "BREAST&0.6697 ($\\pm0.11$)&0.6169 ($\\pm0.11$)&0.6140 ($\\pm0.09$)&0.6359 ($\\pm0.14$)&0.5896 ($\\pm0.09$)&0.7006 ($\\pm0.16$)&0.6828 ($\\pm0.15$)&0.7088 ($\\pm0.10$)\\\\\n",
      "CREDIT&0.9177 ($\\pm0.03$)&0.8962 ($\\pm0.03$)&0.8686 ($\\pm0.03$)&0.9243 ($\\pm0.05$)&0.8116 ($\\pm0.04$)&0.9101 ($\\pm0.05$)&0.9350 ($\\pm0.02$)&0.9196 ($\\pm0.03$)\\\\\n",
      "CYLINDER&0.7346 ($\\pm0.09$)&0.5471 ($\\pm0.05$)&0.7027 ($\\pm0.09$)&0.7775 ($\\pm0.07$)&0.5944 ($\\pm0.09$)&0.8443 ($\\pm0.07$)&0.8697 ($\\pm0.04$)&0.7590 ($\\pm0.10$)\\\\\n",
      "DIABETES&0.8303 ($\\pm0.04$)&0.7985 ($\\pm0.06$)&0.7240 ($\\pm0.05$)&0.8289 ($\\pm0.03$)&0.6733 ($\\pm0.04$)&0.8425 ($\\pm0.02$)&0.8261 ($\\pm0.07$)&0.8356 ($\\pm0.05$)\\\\\n",
      "IONOSPHERE&0.8944 ($\\pm0.05$)&0.8884 ($\\pm0.08$)&0.8548 ($\\pm0.06$)&0.9007 ($\\pm0.09$)&0.8918 ($\\pm0.06$)&0.9851 ($\\pm0.01$)&0.9789 ($\\pm0.02$)&0.9792 ($\\pm0.02$)\\\\\n",
      "OCCUPANCY&0.9961 ($\\pm0.00$)&0.9935 ($\\pm0.00$)&0.9903 ($\\pm0.00$)&0.9943 ($\\pm0.00$)&0.9888 ($\\pm0.00$)&0.9972 ($\\pm0.00$)&0.9990 ($\\pm0.00$)&0.9929 ($\\pm0.00$)\\\\\n",
      "SONAR&0.7802 ($\\pm0.10$)&0.7621 ($\\pm0.11$)&0.7672 ($\\pm0.08$)&0.8026 ($\\pm0.13$)&0.7307 ($\\pm0.11$)&0.9272 ($\\pm0.07$)&0.9170 ($\\pm0.04$)&0.9161 ($\\pm0.05$)\\\\\n",
      "\\midrule\n",
      "Average Score&0.8569&0.8101&0.8101&0.8628&0.7766&0.9009&0.8987&0.8899\\\\\n",
      "Average Rank&4.00&6.44&6.78&3.56&7.67&1.89&2.78&2.78\\\\\n"
     ]
    }
   ],
   "source": [
    "dataset_grouped = df.groupby([\"dataset\"])\n",
    "all_scores = []\n",
    "all_stds = []\n",
    "for dataset_group in dataset_grouped:\n",
    "    scores = {}\n",
    "    stds = {}\n",
    "    dataset = dataset_group[0][0]\n",
    "    if dataset == \"BANANA\" or (dataset.startswith(\"AD0\")) or dataset == \"HEPATITIS\" or dataset == \"WISCONSIN\":\n",
    "        continue\n",
    "    for classifier_group in dataset_group[1].groupby([\"classifier\"]):\n",
    "        number_of_folds_done = len(classifier_group[1])\n",
    "\n",
    "        classifier = classifier_group[0][0]\n",
    "        mean_auc = classifier_group[1][\"auc_score\"].mean()\n",
    "        std_auc = classifier_group[1][\"auc_score\"].std()\n",
    "        mean_acc = classifier_group[1][\"accuracy_score\"].mean()\n",
    "        std_acc = classifier_group[1][\"accuracy_score\"].std()\n",
    "        scores[classifier] = (mean_auc, mean_acc)\n",
    "        stds[classifier] = (std_auc, std_acc)\n",
    "        \n",
    "\n",
    "\n",
    "    # Get the ranks\n",
    "    for classifier in classifiers:\n",
    "        if classifier not in scores:\n",
    "            scores[classifier] = (0, 0)\n",
    "            stds[classifier] = (0, 0)\n",
    "\n",
    "    for classifier in classifiers:\n",
    "        current_score = scores[classifier][score_metric_index]\n",
    "        rank = 1\n",
    "        for other_classifier in classifiers:\n",
    "            if other_classifier == classifier:\n",
    "                continue\n",
    "            if scores[other_classifier][score_metric_index] > current_score:\n",
    "                rank += 1\n",
    "        scores[classifier] = (scores[classifier][0], scores[classifier][1], rank)\n",
    "    \n",
    "\n",
    "    print(f\"{dataset}\", end=\"&\")\n",
    "    if score_metric == \"auc_score\":\n",
    "        # Print AUC scores\n",
    "        for clf in classifiers:\n",
    "            print(f\"{scores[clf][0]:.4f} ($\\\\pm{stds[clf][0]:.2f}$)\", end=\"&\")\n",
    "\n",
    "    if score_metric == \"accuracy_score\":\n",
    "        # Print Acc\n",
    "        for clf in classifiers:\n",
    "            print(f\"{scores[clf][1]:.4f} ($\\\\pm{stds[clf][1]:.2f}$)\", end=\"&\")\n",
    "    \n",
    "    # # Print rank\n",
    "    # for clf in classifiers:\n",
    "    #     print(f\"{scores[clf][2]}\", end=\"&\")\n",
    "    \n",
    "    print(f\"\\b\\\\\\\\\")\n",
    "\n",
    "    all_scores.append(scores)\n",
    "    all_stds.append(stds)\n",
    "    # print(f\"{dataset} & {scores['EDC'][0]:.4f} & {scores['random_forest'][0]:.4f} & {scores['EDC'][1]:.4f} & {scores['random_forest'][1]:.4f} \\\\\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "# Get average ranks\n",
    "average_ranks = {}\n",
    "average_aucs = {}\n",
    "average_accs = {}\n",
    "for classifier in classifiers:\n",
    "    average_ranks[classifier] = 0\n",
    "    average_aucs[classifier] = 0\n",
    "    average_accs[classifier] = 0\n",
    "    for dataset in all_scores:\n",
    "        if classifier not in dataset:\n",
    "            continue\n",
    "        average_ranks[classifier] += dataset[classifier][2]\n",
    "        average_aucs[classifier] += dataset[classifier][0]\n",
    "        average_accs[classifier] += dataset[classifier][1]\n",
    "    average_ranks[classifier] /= len(all_scores)\n",
    "    average_aucs[classifier] /= len(all_scores)\n",
    "    average_accs[classifier] /= len(all_scores)\n",
    "\n",
    "print(\"\\\\midrule\")\n",
    "print(f\"Average Score&\", end=\"\")\n",
    "for classifier in classifiers:\n",
    "    print(f\"{average_aucs[classifier]:.4f}\", end=\"&\")\n",
    "print(f\"\\b\\\\\\\\\")\n",
    "\n",
    "print(f\"Average Rank&\", end=\"\")\n",
    "for classifier in classifiers:\n",
    "    print(f\"{average_ranks[classifier]:.2f}\", end=\"&\")\n",
    "print(f\"\\b\\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
