{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\"EDC\", \"random_forest\", \"logistic_regression\", \"svm_linear\", \"svm_rbf\", \"decision_tree\"]\n",
    "score_metric = \"auc_score\"\n",
    "score_metric_index = 0 if score_metric == \"auc_score\" else 1\n",
    "\n",
    "result_files = os.listdir(\"results\")\n",
    "results = []\n",
    "for result_file in result_files:\n",
    "    dataset, search_strategy, optimiser, random_seed = result_file.split(\n",
    "        \".\"\n",
    "    )[0].split(\"-\")\n",
    "    with open(f\"results/{result_file}\", \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "    for elapsed_time, auc_score, accuracy_score in zip(res[\"elapsed_time\"], res[\"auc_score\"], res[\"accuracy_score\"]):\n",
    "        results.append(\n",
    "            [\n",
    "                dataset,\n",
    "                \"EDC\",  # \"classifier\n",
    "                search_strategy,\n",
    "                optimiser,\n",
    "                elapsed_time,\n",
    "                auc_score,\n",
    "                accuracy_score,\n",
    "                random_seed,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "result_files = os.listdir(\"comparison_results\")\n",
    "for result_file in result_files:\n",
    "    dataset, classifier, random_seed = result_file.split(\n",
    "        \".\"\n",
    "    )[0].split(\"-\")\n",
    "    with open(f\"comparison_results/{result_file}\", \"rb\") as f:\n",
    "        res = pickle.load(f)\n",
    "    for elapsed_time, auc_score, accuracy_score in zip(res[\"elapsed_time\"], res[\"auc_score\"], res[\"accuracy_score\"]):\n",
    "        results.append(\n",
    "            [\n",
    "                dataset,\n",
    "                classifier,  # \"classifier\n",
    "                None,\n",
    "                None,\n",
    "                elapsed_time,\n",
    "                auc_score,\n",
    "                accuracy_score,\n",
    "                random_seed,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"dataset\",\n",
    "        \"classifier\",\n",
    "        \"search_strategy\",\n",
    "        \"optimiser\",\n",
    "        \"elapsed_time\",\n",
    "        \"auc_score\",\n",
    "        \"accuracy_score\",\n",
    "        \"random_seed\"\n",
    "    ],\n",
    "    data=results,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADULT&0.8800 ($\\pm0.01$)&0.8884 ($\\pm0.00$)&0.6392 ($\\pm0.01$)&0.8534 ($\\pm0.12$)&0.6029 ($\\pm0.07$)&0.7262 ($\\pm0.01$)\\\\\n",
      "BANKNOTE&0.9998 ($\\pm0.00$)&0.9998 ($\\pm0.00$)&0.9993 ($\\pm0.00$)&0.9995 ($\\pm0.00$)&1.0000 ($\\pm0.00$)&0.9904 ($\\pm0.01$)\\\\\n",
      "BREAST&0.6315 ($\\pm0.15$)&0.6444 ($\\pm0.10$)&0.7009 ($\\pm0.07$)&0.6740 ($\\pm0.09$)&0.7105 ($\\pm0.10$)&0.6061 ($\\pm0.10$)\\\\\n",
      "CREDIT&0.9176 ($\\pm0.04$)&0.9351 ($\\pm0.02$)&0.7512 ($\\pm0.07$)&0.7611 ($\\pm0.06$)&0.7805 ($\\pm0.05$)&0.8226 ($\\pm0.05$)\\\\\n",
      "CYLINDER&0.7324 ($\\pm0.09$)&0.8489 ($\\pm0.06$)&0.5647 ($\\pm0.08$)&0.5869 ($\\pm0.08$)&0.6030 ($\\pm0.10$)&0.7029 ($\\pm0.09$)\\\\\n",
      "DIABETES&0.8347 ($\\pm0.04$)&0.7287 ($\\pm0.05$)&0.6768 ($\\pm0.05$)&0.6998 ($\\pm0.05$)&0.6904 ($\\pm0.06$)&0.6297 ($\\pm0.07$)\\\\\n",
      "HEPATITIS&0.7949 ($\\pm0.25$)&0.8622 ($\\pm0.16$)&0.7970 ($\\pm0.13$)&0.7546 ($\\pm0.35$)&0.8190 ($\\pm0.16$)&0.5807 ($\\pm0.24$)\\\\\n",
      "IONOSPHERE&0.8833 ($\\pm0.07$)&0.9802 ($\\pm0.02$)&0.9360 ($\\pm0.04$)&0.9462 ($\\pm0.06$)&0.9849 ($\\pm0.02$)&0.8856 ($\\pm0.04$)\\\\\n",
      "OCCUPANCY&0.9958 ($\\pm0.00$)&0.9978 ($\\pm0.00$)&0.9761 ($\\pm0.00$)&0.9895 ($\\pm0.00$)&0.9922 ($\\pm0.00$)&0.9820 ($\\pm0.00$)\\\\\n",
      "SONAR&0.7787 ($\\pm0.12$)&0.9363 ($\\pm0.06$)&0.8084 ($\\pm0.16$)&0.7906 ($\\pm0.15$)&0.8915 ($\\pm0.05$)&0.7694 ($\\pm0.10$)\\\\\n",
      "WISCONSIN&0.5548 ($\\pm0.13$)&0.5606 ($\\pm0.13$)&0.6079 ($\\pm0.12$)&0.4853 ($\\pm0.21$)&0.4799 ($\\pm0.14$)&0.5321 ($\\pm0.12$)\\\\\n",
      "\\midrule\n",
      "Average Score&0.8185&0.8529&0.7689&0.7764&0.7777&0.7480\\\\\n",
      "Average Rank&3.18&1.64&4.18&4.00&3.09&4.91\\\\\n"
     ]
    }
   ],
   "source": [
    "dataset_grouped = df.groupby([\"dataset\"])\n",
    "all_scores = []\n",
    "all_stds = []\n",
    "for dataset_group in dataset_grouped:\n",
    "    scores = {}\n",
    "    stds = {}\n",
    "    dataset = dataset_group[0][0]\n",
    "    if dataset == \"BANANA\" or (dataset.startswith(\"AD0\")):\n",
    "        continue\n",
    "    for classifier_group in dataset_group[1].groupby([\"classifier\"]):\n",
    "        number_of_folds_done = len(classifier_group[1])\n",
    "\n",
    "        classifier = classifier_group[0][0]\n",
    "        mean_auc = classifier_group[1][\"auc_score\"].mean()\n",
    "        std_auc = classifier_group[1][\"auc_score\"].std()\n",
    "        mean_acc = classifier_group[1][\"accuracy_score\"].mean()\n",
    "        std_acc = classifier_group[1][\"accuracy_score\"].std()\n",
    "        scores[classifier] = (mean_auc, mean_acc)\n",
    "        stds[classifier] = (std_auc, std_acc)\n",
    "        \n",
    "\n",
    "\n",
    "    # Get the ranks\n",
    "    for classifier in classifiers:\n",
    "        current_score = scores[classifier][score_metric_index]\n",
    "        rank = 1\n",
    "        for other_classifier in classifiers:\n",
    "            if other_classifier == classifier:\n",
    "                continue\n",
    "            if scores[other_classifier][score_metric_index] > current_score:\n",
    "                rank += 1\n",
    "        scores[classifier] = (scores[classifier][0], scores[classifier][1], rank)\n",
    "    \n",
    "\n",
    "    print(f\"{dataset}\", end=\"&\")\n",
    "    if score_metric == \"auc_score\":\n",
    "        # Print AUC scores\n",
    "        for clf in classifiers:\n",
    "            print(f\"{scores[clf][0]:.4f} ($\\\\pm{stds[clf][0]:.2f}$)\", end=\"&\")\n",
    "\n",
    "    if score_metric == \"accuracy_score\":\n",
    "        # Print Acc\n",
    "        for clf in classifiers:\n",
    "            print(f\"{scores[clf][1]:.4f} ($\\\\pm{stds[clf][1]:.2f}$)\", end=\"&\")\n",
    "    \n",
    "    # # Print rank\n",
    "    # for clf in classifiers:\n",
    "    #     print(f\"{scores[clf][2]}\", end=\"&\")\n",
    "    \n",
    "    print(f\"\\b\\\\\\\\\")\n",
    "\n",
    "    all_scores.append(scores)\n",
    "    all_stds.append(stds)\n",
    "    # print(f\"{dataset} & {scores['EDC'][0]:.4f} & {scores['random_forest'][0]:.4f} & {scores['EDC'][1]:.4f} & {scores['random_forest'][1]:.4f} \\\\\\\\\")\n",
    "\n",
    "\n",
    "\n",
    "# Get average ranks\n",
    "average_ranks = {}\n",
    "average_aucs = {}\n",
    "average_accs = {}\n",
    "for classifier in classifiers:\n",
    "    average_ranks[classifier] = 0\n",
    "    average_aucs[classifier] = 0\n",
    "    average_accs[classifier] = 0\n",
    "    for dataset in all_scores:\n",
    "        if classifier not in dataset:\n",
    "            continue\n",
    "        average_ranks[classifier] += dataset[classifier][2]\n",
    "        average_aucs[classifier] += dataset[classifier][0]\n",
    "        average_accs[classifier] += dataset[classifier][1]\n",
    "    average_ranks[classifier] /= len(all_scores)\n",
    "    average_aucs[classifier] /= len(all_scores)\n",
    "    average_accs[classifier] /= len(all_scores)\n",
    "\n",
    "print(\"\\\\midrule\")\n",
    "print(f\"Average Score&\", end=\"\")\n",
    "for classifier in classifiers:\n",
    "    print(f\"{average_aucs[classifier]:.4f}\", end=\"&\")\n",
    "print(f\"\\b\\\\\\\\\")\n",
    "\n",
    "print(f\"Average Rank&\", end=\"\")\n",
    "for classifier in classifiers:\n",
    "    print(f\"{average_ranks[classifier]:.2f}\", end=\"&\")\n",
    "print(f\"\\b\\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
